# @package _global_

train:
  epochs: 100
  gradient_accumulation_steps: 1
  gradient_clip: 1.0
  mixed_precision: "no"  # Can be "fp16" or "bf16"
  
  optimizer:
    lr: 1e-3
    betas: [0.9, 0.999]
    weight_decay: 0.01
  
  scheduler:
    enabled: true
    eta_min: 1e-5