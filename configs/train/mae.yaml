# @package _global_

train:
  # Training parameters
  epochs: 100
  mask_ratio: 0.85
  gradient_clip: 1.0
  
  # Optimizer
  optimizer:
    name: AdamW
    lr: 8e-5
    betas: [0.9, 0.999]
    weight_decay: 0.001
  
  # Scheduler
  scheduler:
    name: CosineAnnealingWarmRestarts
    T_0: 10
    T_mult: 1
    eta_min: 1e-6
  
  # Loss
  loss:
    name: mse
    reduction: mean
  
  # Resume training
  resume: true  # Automatically resume from latest checkpoint if available